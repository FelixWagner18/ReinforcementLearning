\relax 
\providecommand*\new@tpo@label[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Categorization of Machine Learning}{1}}
\citation{SuttonBarto}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Agent-Environment Interaction}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces  \cite  {SuttonBarto} The Interaction of agent and environment build a trajectory of states, actions and rewards at different time steps: $S_0 \rightarrow A_0 \rightarrow \{ R_{1}, S_1 \} \rightarrow A_1 \rightarrow \{ R_{2}, S_2 \} \rightarrow A_2 ...$\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:agentenv}{{1.1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Policies and Values}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Winning Tic-Tac-Toe}{3}}
\citation{SuttonBarto}
\citation{ShangtongZhang}
\citation{ShangtongZhang}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Remark about Complex Real World Problems}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Multi Armed Bandit Problem}{4}}
\newlabel{eq:estactval}{{2.1}{4}}
\citation{SuttonBarto}
\citation{ShangtongZhang}
\citation{ShangtongZhang}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \cite  {ShangtongZhang} The rewards for every action are normally distributed and their expactations $\mu _i$ equal the real action values $q_{i,*}$, with a standard deviation of $\sigma _i = 1$. The real action values themselves are also normally distributed with expactation $\mu = 0$ and standard deviation $\sigma = 1$.\relax }}{5}}
\newlabel{fig:rewarddist}{{2.1}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}$\epsilon $-greedy Method}{5}}
\citation{ShangtongZhang}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces \cite  {ShangtongZhang} Average rewards of the $\epsilon $-greedy method, for different values of $\epsilon $ and the first 1000 time steps. It can be seen, that the blue line, which corresponds to $\epsilon = 0$, has the highest average rewards in the first 50 to 100 time steps.\relax }}{6}}
\newlabel{fig:epsilongreedy}{{2.2}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Upper Confidence Bound Action Selection}{6}}
\citation{SuttonBarto}
\citation{ShangtongZhang}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \cite  {ShangtongZhang} Learning curves of the UCB method and the $\epsilon $-greedy method, in the first 1000 time steps.\relax }}{7}}
\newlabel{fig:UCB}{{2.3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Comparison}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces \cite  {ShangtongZhang} Parameter study of four different methods of policy learning.\relax }}{7}}
\newlabel{fig:comparison}{{2.4}{7}}
\citation{WikiMarkov}
\@writefile{toc}{\contentsline {section}{\numberline {3}Finite Markov Decision Processes}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \cite  {WikiMarkov} Visualization of a Markov Decision Process with three states, two possible actions and many different rewards, depending of the state and action.\relax }}{8}}
\newlabel{fig:MDP}{{3.1}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Continuing Tasks}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Bellman Equation}{9}}
\citation{SuttonBarto}
\citation{SuttonBarto}
\citation{SuttonBarto}
\newlabel{bell}{{3.4}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Bellman Optimality Equation}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Gridworld with random and optimal policy}{10}}
\citation{SuttonBarto}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces \cite  {SuttonBarto} Left: The field of gridworld. Two states are jump-and-reward states. Right: The values of the states when using a random policy.\relax }}{11}}
\newlabel{fig:RandomGridworld}{{3.2}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces \cite  {SuttonBarto} Gridworld with an optimal policy. This policy is not unique, as it can be seen in the right part of the picture.\relax }}{11}}
\newlabel{fig:OptimalGridworld}{{3.3}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Project: Escape the Labyrinth}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The agent has to find the fastest way out of the labyrinth. He starts at the red cross.\relax }}{11}}
\newlabel{fig:labyrinth}{{4.1}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Results}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Learning curve of the agent in the labyrinth escape. After 30 episodes an optimal policy is found, which enables the agent to exit the labyrinth with 11 steps.\relax }}{12}}
\newlabel{fig:LabLearning}{{4.2}{12}}
\bibstyle{plain}
\bibdata{wagner}
\bibcite{ShangtongZhang}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Learned q-table with the optimal policy.\relax }}{13}}
\newlabel{fig:LearnedQTables}{{4.3}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Optimal Actions from every state, to leave the labyrinth on the most direct way.\relax }}{13}}
\newlabel{fig:OptimalAction}{{4.4}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Discussion}{13}}
\bibcite{SuttonBarto}{2}
\bibcite{WikiMarkov}{3}
